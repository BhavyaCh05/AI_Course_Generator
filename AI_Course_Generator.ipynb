{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQ2ORzTSwrzD"
   },
   "source": [
    "# **1. Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UZ9Bb4-Wmzho",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.7.1-py3-none-any.whl (163 kB)\n",
      "     -------------------------------------- 163.9/163.9 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting google-api-core\n",
      "  Downloading google_api_core-2.19.1-py3-none-any.whl (139 kB)\n",
      "     -------------------------------------- 139.4/139.4 kB 8.1 MB/s eta 0:00:00\n",
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.136.0-py2.py3-none-any.whl (11.9 MB)\n",
      "     ---------------------------------------- 11.9/11.9 MB 6.5 MB/s eta 0:00:00\n",
      "Collecting google-ai-generativelanguage==0.6.6\n",
      "  Downloading google_ai_generativelanguage-0.6.6-py3-none-any.whl (718 kB)\n",
      "     -------------------------------------- 718.3/718.3 kB 7.5 MB/s eta 0:00:00\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-2.8.0-py3-none-any.whl (423 kB)\n",
      "     -------------------------------------- 423.1/423.1 kB 8.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from google-generativeai) (4.64.1)\n",
      "Collecting google-auth>=2.15.0\n",
      "  Downloading google_auth-2.31.0-py2.py3-none-any.whl (194 kB)\n",
      "     -------------------------------------- 194.6/194.6 kB 5.9 MB/s eta 0:00:00\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-5.27.2-cp39-cp39-win_amd64.whl (426 kB)\n",
      "     -------------------------------------- 426.9/426.9 kB 8.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from google-generativeai) (4.3.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3\n",
      "  Downloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-4.25.3-cp39-cp39-win_amd64.whl (413 kB)\n",
      "     -------------------------------------- 413.4/413.4 kB 8.6 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (2.28.1)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "     -------------------------------------- 220.0/220.0 kB 6.8 MB/s eta 0:00:00\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.9/96.9 kB 5.8 MB/s eta 0:00:00\n",
      "Collecting uritemplate<5,>=3.0.1\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pydantic-core==2.20.0\n",
      "  Downloading pydantic_core-2.20.0-cp39-none-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 8.7 MB/s eta 0:00:00\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai) (0.4.5)\n",
      "Collecting grpcio<2.0dev,>=1.33.2\n",
      "  Downloading grpcio-1.64.1-cp39-cp39-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 7.9 MB/s eta 0:00:00\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (1.26.11)\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.62.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: uritemplate, typing-extensions, rsa, protobuf, httplib2, grpcio, cachetools, annotated-types, pydantic-core, proto-plus, googleapis-common-protos, google-auth, pydantic, grpcio-status, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "Successfully installed annotated-types-0.7.0 cachetools-5.3.3 google-ai-generativelanguage-0.6.6 google-api-core-2.19.1 google-api-python-client-2.136.0 google-auth-2.31.0 google-auth-httplib2-0.2.0 google-generativeai-0.7.1 googleapis-common-protos-1.63.2 grpcio-1.64.1 grpcio-status-1.62.2 httplib2-0.22.0 proto-plus-1.24.0 protobuf-4.25.3 pydantic-2.8.0 pydantic-core-2.20.0 rsa-4.9 typing-extensions-4.12.2 uritemplate-4.1.1\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "     ---------------------------------------- 9.3/9.3 MB 4.6 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.3-cp39-none-win_amd64.whl (287 kB)\n",
      "     -------------------------------------- 287.9/287.9 kB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "     -------------------------------------- 402.6/402.6 kB 6.2 MB/s eta 0:00:00\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "     -------------------------------------- 177.6/177.6 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "Successfully installed fsspec-2024.6.1 huggingface-hub-0.23.4 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.42.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tika\n",
      "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: setuptools in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from tika) (63.4.1)\n",
      "Requirement already satisfied: requests in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from tika) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from requests->tika) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from requests->tika) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from requests->tika) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from requests->tika) (1.26.11)\n",
      "Building wheels for collected packages: tika\n",
      "  Building wheel for tika (setup.py): started\n",
      "  Building wheel for tika (setup.py): finished with status 'done'\n",
      "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32624 sha256=bb4fdb9f90648cbe1982eb1b2b660d634e09710cf05d3b1e6ba00d8e1d223361\n",
      "  Stored in directory: c:\\users\\bhavy\\appdata\\local\\pip\\cache\\wheels\\13\\56\\18\\e752060632d32c39c9c4545e756dad281f8504dafcfac02b95\n",
      "Successfully built tika\n",
      "Installing collected packages: tika\n",
      "Successfully installed tika-2.6.0\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "     -------------------------------------- 232.6/232.6 kB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from PyPDF2) (4.12.2)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Collecting reportlab\n",
      "  Downloading reportlab-4.2.2-py3-none-any.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: chardet in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from reportlab) (4.0.0)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from reportlab) (9.2.0)\n",
      "Installing collected packages: reportlab\n",
      "Successfully installed reportlab-4.2.2\n",
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting httpx==0.13.3\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 55.1/55.1 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting httpcore==0.9.*\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.6/42.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2022.9.14)\n",
      "Requirement already satisfied: sniffio in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
      "Collecting idna==2.*\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.8/58.8 kB ? eta 0:00:00\n",
      "Collecting rfc3986<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting hstspreload\n",
      "  Downloading hstspreload-2024.7.1-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 3.9 MB/s eta 0:00:00\n",
      "Collecting chardet==3.*\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.4/133.4 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting h2==3.*\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.0/65.0 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting h11<0.10,>=0.8\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 53.6/53.6 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17411 sha256=8fc42c143ede41f98cf75432d33a726b2f2d1aa096fd94f55f03dbff8c8edd75\n",
      "  Stored in directory: c:\\users\\bhavy\\appdata\\local\\pip\\cache\\wheels\\60\\b3\\27\\d8aff3e2d5c2d0d97a117cdf0d5f13cd121e2c2b5fb49b55a0\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.3\n",
      "    Uninstalling idna-3.3:\n",
      "      Successfully uninstalled idna-3.3\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.7.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.2.2 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.2.2 requires pyqtwebengine<5.13, which is not installed.\n",
      "anaconda-project 0.11.1 requires ruamel-yaml, which is not installed.\n",
      "conda-repo-cli 1.0.20 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires nbformat==5.4.0, but you have nbformat 5.5.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai\n",
    "!pip install transformers\n",
    "!pip install tika\n",
    "!pip install PyPDF2\n",
    "!pip install reportlab\n",
    "!pip install googletrans==4.0.0-rc1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pWTXvTkfudR-",
    "outputId": "7ed8e2e9-41c6-40e3-906d-8457f6bcfeba",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the path to your PDF file: /content/new_resume.pdf\n",
      "Choose an option: (1) Generate Course, (2) Generate Questions, (3) Talk to the PDF: 3\n",
      "You can now talk to the PDF. Ask any questions based on its content.\n",
      "Your question (type 'exit' to quit): what is this all about\n",
      "Response:\n",
      "Unfortunately, I do not have access to any reference text, so I am unable to answer this question from the provided context. This document is a resume of Abhishek Sinha, an AI engineer with experience in machine learning, deep learning, and cloud computing. He has worked on projects such as an AI-based multilingual course generator and has experience with Google Cloud, AWS, and Azure. His skills include prompt engineering, generative AI, working with LLM, API integration, AI systems, and more. He has a Bachelor of Engineering from Acharya Institute of Technology and an Executive Post Graduate Certification in Data Science & Artificial Intelligence from iHub, Divya Sampark IIT Roorkee.\n",
      "\n",
      "Your question (type 'exit' to quit): exit\n",
      "Do you want to perform another operation? (yes/no): yes\n",
      "Choose an option: (1) Generate Course, (2) Generate Questions, (3) Talk to the PDF: 1\n",
      "Course Outline and Lesson Plan (in English):\n",
      "**Chapter 1: Introduction to Artificial Intelligence (AI)**\n",
      "\n",
      "**Learning Objectives:**\n",
      "* Understand the fundamental concepts and history of AI.\n",
      "* Identify the different types of AI and their applications.\n",
      "* Discuss the ethical implications and future prospects of AI.\n",
      "\n",
      "**Chapter Outline:**\n",
      "* Definition and evolution of AI\n",
      "* Types of AI: Machine Learning, Deep Learning, Natural Language Processing\n",
      "* Applications of AI in various industries\n",
      "* Ethical considerations in AI development\n",
      "* Future trends and advancements in AI\n",
      "\n",
      "**Chapter 2: Machine Learning (ML) Fundamentals**\n",
      "\n",
      "**Learning Objectives:**\n",
      "* Gain a comprehensive understanding of ML concepts and algorithms.\n",
      "* Apply ML techniques to solve real-world problems.\n",
      "* Evaluate and optimize ML models.\n",
      "\n",
      "**Chapter Outline:**\n",
      "* Types of ML: Supervised, Unsupervised, Reinforcement Learning\n",
      "* ML algorithms: Regression, Classification, Clustering\n",
      "* Model evaluation metrics and optimization techniques\n",
      "* Case study: Building an ML model for spam detection\n",
      "\n",
      "**Chapter 3: Deep Learning (DL)**\n",
      "\n",
      "**Learning Objectives:**\n",
      "* Master the principles and architectures of DL networks.\n",
      "* Apply DL techniques to solve complex problems.\n",
      "* Implement DL models using popular frameworks.\n",
      "\n",
      "**Chapter Outline:**\n",
      "* Artificial Neural Networks: Convolutional Neural Networks, Recurrent Neural Networks\n",
      "* DL architectures: VGGNet, ResNet, Transformer\n",
      "* DL algorithms: Backpropagation, Gradient Descent\n",
      "* Case study: Building a DL model for image recognition\n",
      "\n",
      "**Chapter 4: Natural Language Processing (NLP)**\n",
      "\n",
      "**Learning Objectives:**\n",
      "* Understand the basics of NLP and its importance in AI.\n",
      "* Apply NLP techniques for text analysis, generation, and translation.\n",
      "* Build NLP models using state-of-the-art tools.\n",
      "\n",
      "**Chapter Outline:**\n",
      "* Text Preprocessing: Tokenization, Stemming, Lemmatization\n",
      "* NLP tasks: Named Entity Recognition, Part-of-Speech Tagging, Machine Translation\n",
      "* NLP models: Transformers, BERT, GPT-3\n",
      "* Case study: Building an NLP model for sentiment analysis\n",
      "\n",
      "**Chapter 5: AI Engineering and Deployment**\n",
      "\n",
      "**Learning Objectives:**\n",
      "* Gain insights into the process of developing and deploying AI systems.\n",
      "* Understand the challenges and best practices involved in AI engineering.\n",
      "* Implement AI solutions in production environments.\n",
      "\n",
      "**Chapter Outline:**\n",
      "* AI development lifecycle: Data Acquisition, Model Training, Evaluation, Deployment\n",
      "* AI infrastructure: Cloud Computing, GPUs\n",
      "* Model monitoring and maintenance\n",
      "* Case study: Deployment of an AI chatbox on a website\n",
      "\n",
      "**Chapter 6: AI Applications in Various Domains**\n",
      "\n",
      "**Learning Objectives:**\n",
      "* Explore the applications of AI in different industries.\n",
      "* Understand the challenges and opportunities in each domain.\n",
      "* Discuss the future of AI in various sectors.\n",
      "\n",
      "**Chapter Outline:**\n",
      "* AI in Healthcare: Disease diagnosis, Drug discovery, Personalized medicine\n",
      "* AI in Finance: Fraud detection, Risk assessment, Investment analysis\n",
      "* AI in Manufacturing: Process optimization, Quality control, Predictive maintenance\n",
      "* AI in Education: Personalized learning, Adaptive content, Virtual assistants\n",
      "\n",
      "**Chapter 7: Ethics and Responsible AI**\n",
      "\n",
      "**Learning Objectives:**\n",
      "* Understand the ethical considerations and challenges in AI development.\n",
      "* Develop guidelines for responsible AI practices.\n",
      "* Promote ethical AI adoption and usage.\n",
      "\n",
      "**Chapter Outline:**\n",
      "* Bias and discrimination in AI\n",
      "* Privacy and data security\n",
      "* Accountability and transparency\n",
      "* Regulation and governance of AI\n",
      "\n",
      "**Guidance for Teachers:**\n",
      "* Utilize real-world examples and case studies to illustrate concepts.\n",
      "* Facilitate group discussions and hands-on projects to foster active learning.\n",
      "* Collaborate with industry experts for guest lectures and project mentoring.\n",
      "* Provide opportunities for students to present their AI research and projects.\n",
      "\n",
      "**Interactive Elements and Multimedia Content:**\n",
      "* Interactive simulations and visualizations to demonstrate AI algorithms.\n",
      "* Online quizzes and assessments to track student progress.\n",
      "* Videos and documentaries on key AI concepts and applications.\n",
      "* Case studies and industry examples to showcase real-world AI solutions.\n",
      "Do you want to translate the course outline? (yes/no): yes\n",
      "Available languages:\n",
      "af (afrikaans)\n",
      "sq (albanian)\n",
      "am (amharic)\n",
      "ar (arabic)\n",
      "hy (armenian)\n",
      "az (azerbaijani)\n",
      "eu (basque)\n",
      "be (belarusian)\n",
      "bn (bengali)\n",
      "bs (bosnian)\n",
      "bg (bulgarian)\n",
      "ca (catalan)\n",
      "ceb (cebuano)\n",
      "ny (chichewa)\n",
      "zh-cn (chinese (simplified))\n",
      "zh-tw (chinese (traditional))\n",
      "co (corsican)\n",
      "hr (croatian)\n",
      "cs (czech)\n",
      "da (danish)\n",
      "nl (dutch)\n",
      "en (english)\n",
      "eo (esperanto)\n",
      "et (estonian)\n",
      "tl (filipino)\n",
      "fi (finnish)\n",
      "fr (french)\n",
      "fy (frisian)\n",
      "gl (galician)\n",
      "ka (georgian)\n",
      "de (german)\n",
      "el (greek)\n",
      "gu (gujarati)\n",
      "ht (haitian creole)\n",
      "ha (hausa)\n",
      "haw (hawaiian)\n",
      "he (hebrew)\n",
      "hi (hindi)\n",
      "hmn (hmong)\n",
      "hu (hungarian)\n",
      "is (icelandic)\n",
      "ig (igbo)\n",
      "id (indonesian)\n",
      "ga (irish)\n",
      "it (italian)\n",
      "ja (japanese)\n",
      "jw (javanese)\n",
      "kn (kannada)\n",
      "kk (kazakh)\n",
      "km (khmer)\n",
      "ko (korean)\n",
      "ku (kurdish (kurmanji))\n",
      "ky (kyrgyz)\n",
      "lo (lao)\n",
      "la (latin)\n",
      "lv (latvian)\n",
      "lt (lithuanian)\n",
      "lb (luxembourgish)\n",
      "mk (macedonian)\n",
      "mg (malagasy)\n",
      "ms (malay)\n",
      "ml (malayalam)\n",
      "mt (maltese)\n",
      "mi (maori)\n",
      "mr (marathi)\n",
      "mn (mongolian)\n",
      "my (myanmar (burmese))\n",
      "ne (nepali)\n",
      "no (norwegian)\n",
      "or (odia)\n",
      "ps (pashto)\n",
      "fa (persian)\n",
      "pl (polish)\n",
      "pt (portuguese)\n",
      "pa (punjabi)\n",
      "ro (romanian)\n",
      "ru (russian)\n",
      "sm (samoan)\n",
      "gd (scots gaelic)\n",
      "sr (serbian)\n",
      "st (sesotho)\n",
      "sn (shona)\n",
      "sd (sindhi)\n",
      "si (sinhala)\n",
      "sk (slovak)\n",
      "sl (slovenian)\n",
      "so (somali)\n",
      "es (spanish)\n",
      "su (sundanese)\n",
      "sw (swahili)\n",
      "sv (swedish)\n",
      "tg (tajik)\n",
      "ta (tamil)\n",
      "te (telugu)\n",
      "th (thai)\n",
      "tr (turkish)\n",
      "uk (ukrainian)\n",
      "ur (urdu)\n",
      "ug (uyghur)\n",
      "uz (uzbek)\n",
      "vi (vietnamese)\n",
      "cy (welsh)\n",
      "xh (xhosa)\n",
      "yi (yiddish)\n",
      "yo (yoruba)\n",
      "zu (zulu)\n",
      "Enter the language code you want to translate to: urdu\n",
      "Translated Course Outline (ur):\n",
      "** باب 1: مصنوعی ذہانت کا تعارف (AI) **\n",
      "\n",
      "**سیکھنے کے مقاصد:**\n",
      "* AI کے بنیادی تصورات اور تاریخ کو سمجھیں۔\n",
      "* AI کی مختلف اقسام اور ان کی درخواستوں کی شناخت کریں۔\n",
      "* اے آئی کے اخلاقی مضمرات اور مستقبل کے امکانات پر تبادلہ خیال کریں۔\n",
      "\n",
      "** باب آؤٹ لائن: **\n",
      "* AI کی تعریف اور ارتقاء\n",
      "* اے آئی کی اقسام: مشین لرننگ ، گہری سیکھنے ، قدرتی زبان پروسیسنگ\n",
      "* مختلف صنعتوں میں AI کی درخواستیں\n",
      "* اے آئی کی ترقی میں اخلاقی تحفظات\n",
      "* AI میں مستقبل کے رجحانات اور پیشرفت\n",
      "\n",
      "** باب 2: مشین لرننگ (ایم ایل) بنیادی اصول **\n",
      "\n",
      "**سیکھنے کے مقاصد:**\n",
      "* ایم ایل کے تصورات اور الگورتھم کی ایک جامع تفہیم حاصل کریں۔\n",
      "* حقیقی دنیا کے مسائل حل کرنے کے لئے ایم ایل تکنیک کا اطلاق کریں۔\n",
      "* ایم ایل ماڈلز کا اندازہ اور بہتر بنائیں۔\n",
      "\n",
      "** باب آؤٹ لائن: **\n",
      "* ایم ایل کی اقسام: نگرانی ، غیر نگرانی ، کمک سیکھنے\n",
      "* ایم ایل الگورتھم: رجعت ، درجہ بندی ، کلسٹرنگ\n",
      "* ماڈل کی تشخیص میٹرکس اور اصلاح کی تکنیک\n",
      "* کیس اسٹڈی: اسپام کا پتہ لگانے کے لئے ایک ایم ایل ماڈل بنانا\n",
      "\n",
      "** باب 3: گہری لرننگ (ڈی ایل) **\n",
      "\n",
      "**سیکھنے کے مقاصد:**\n",
      "* ڈی ایل نیٹ ورکس کے اصولوں اور فن تعمیرات پر عبور حاصل کریں۔\n",
      "* پیچیدہ مسائل کو حل کرنے کے لئے DL تکنیک کا اطلاق کریں۔\n",
      "* مقبول فریم ورک کا استعمال کرتے ہوئے ڈی ایل ماڈل کو نافذ کریں۔\n",
      "\n",
      "** باب آؤٹ لائن: **\n",
      "* مصنوعی اعصابی نیٹ ورک: متنازعہ اعصابی نیٹ ورک ، بار بار اعصابی نیٹ ورک\n",
      "* ڈی ایل آرکیٹیکچرز: وی جی جی این ای ٹی ، ریزنیٹ ، ٹرانسفارمر\n",
      "* ڈی ایل الگورتھم: بیک پروپیگیشن ، تدریجی نزول\n",
      "* کیس اسٹڈی: تصویری شناخت کے لئے ڈی ایل ماڈل کی تعمیر\n",
      "\n",
      "** باب 4: قدرتی زبان پروسیسنگ (این ایل پی) **\n",
      "\n",
      "**سیکھنے کے مقاصد:**\n",
      "* این ایل پی کی بنیادی باتوں اور اے آئی میں اس کی اہمیت کو سمجھیں۔\n",
      "* متن کے تجزیہ ، نسل اور ترجمہ کے لئے NLP تکنیک کا اطلاق کریں۔\n",
      "* جدید ترین ٹولز کا استعمال کرتے ہوئے NLP ماڈل بنائیں۔\n",
      "\n",
      "** باب آؤٹ لائن: **\n",
      "* متن پری پروسیسنگ: ٹوکنائزیشن ، اسٹیمنگ ، لیممیٹائزیشن\n",
      "* این ایل پی ٹاسکس: نامزد ہستی کی شناخت ، اسپیچ آف اسپیچ ٹیگنگ ، مشین ترجمہ\n",
      "* این ایل پی ماڈل: ٹرانسفارمر ، برٹ ، جی پی ٹی 3\n",
      "* کیس اسٹڈی: جذبات کے تجزیے کے لئے این ایل پی ماڈل بنانا\n",
      "\n",
      "** باب 5: AI انجینئرنگ اور تعیناتی **\n",
      "\n",
      "**سیکھنے کے مقاصد:**\n",
      "* AI سسٹم کی ترقی اور تعیناتی کے عمل کے بارے میں بصیرت حاصل کریں۔\n",
      "* اے آئی انجینئرنگ میں شامل چیلنجوں اور بہترین طریقوں کو سمجھیں۔\n",
      "* پیداواری ماحول میں AI حل نافذ کریں۔\n",
      "\n",
      "** باب آؤٹ لائن: **\n",
      "* اے آئی ڈویلپمنٹ لائف سائیکل: ڈیٹا کے حصول ، ماڈل کی تربیت ، تشخیص ، تعیناتی\n",
      "* اے آئی انفراسٹرکچر: کلاؤڈ کمپیوٹنگ ، جی پی یو\n",
      "* ماڈل کی نگرانی اور بحالی\n",
      "* کیس اسٹڈی: کسی ویب سائٹ پر AI چیٹ باکس کی تعیناتی\n",
      "\n",
      "** باب 6: مختلف ڈومینز میں AI ایپلی کیشنز **\n",
      "\n",
      "**سیکھنے کے مقاصد:**\n",
      "* مختلف صنعتوں میں AI کی درخواستوں کو دریافت کریں۔\n",
      "* ہر ڈومین میں چیلنجوں اور مواقع کو سمجھیں۔\n",
      "* مختلف شعبوں میں اے آئی کے مستقبل پر تبادلہ خیال کریں۔\n",
      "\n",
      "** باب آؤٹ لائن: **\n",
      "* صحت کی دیکھ بھال میں AI: بیماری کی تشخیص ، منشیات کی دریافت ، ذاتی نوعیت کی دوائی\n",
      "* فنانس میں AI: دھوکہ دہی کا پتہ لگانا ، رسک کی تشخیص ، سرمایہ کاری کا تجزیہ\n",
      "* مینوفیکچرنگ میں AI: عمل کی اصلاح ، کوالٹی کنٹرول ، پیش گوئی کی بحالی\n",
      "* تعلیم میں AI: ذاتی نوعیت کی تعلیم ، انکولی مواد ، ورچوئل اسسٹنٹس\n",
      "\n",
      "** باب 7: اخلاقیات اور ذمہ دار AI **\n",
      "\n",
      "**سیکھنے کے مقاصد:**\n",
      "* اے آئی کی ترقی میں اخلاقی تحفظات اور چیلنجوں کو سمجھیں۔\n",
      "* ذمہ دار AI طریقوں کے لئے رہنما اصول تیار کریں۔\n",
      "* اخلاقی AI کو اپنانے اور استعمال کو فروغ دیں۔\n",
      "\n",
      "** باب آؤٹ لائن: **\n",
      "* AI میں تعصب اور امتیازی سلوک\n",
      "* رازداری اور ڈیٹا سیکیورٹی\n",
      "* احتساب اور شفافیت\n",
      "* AI کی ضابطہ اور حکمرانی\n",
      "\n",
      "** اساتذہ کے لئے رہنمائی: **\n",
      "* تصورات کو واضح کرنے کے لئے حقیقی دنیا کی مثالوں اور کیس اسٹڈیز کا استعمال کریں۔\n",
      "* فعال تعلیم کو فروغ دینے کے لئے گروپ مباحثوں اور ہینڈ آن پروجیکٹس کی سہولت فراہم کریں۔\n",
      "* مہمانوں کے لیکچرز اور پروجیکٹ کی رہنمائی کے لئے صنعت کے ماہرین کے ساتھ تعاون کریں۔\n",
      "* طلباء کو اپنی AI تحقیق اور منصوبوں کو پیش کرنے کے مواقع فراہم کریں۔\n",
      "\n",
      "** انٹرایکٹو عناصر اور ملٹی میڈیا مواد: **\n",
      "* AI الگورتھم کا مظاہرہ کرنے کے لئے انٹرایکٹو تخروپن اور تصورات۔\n",
      "* طلباء کی پیشرفت کو ٹریک کرنے کے لئے آن لائن کوئز اور تشخیص۔\n",
      "* کلیدی AI تصورات اور ایپلی کیشنز پر ویڈیوز اور دستاویزی فلمیں۔\n",
      "* کیس اسٹڈیز اور صنعت کی مثالیں حقیقی دنیا کے AI حل کو ظاہر کرنے کے لئے۔\n",
      "Do you want to perform another operation? (yes/no): yes\n",
      "Choose an option: (1) Generate Course, (2) Generate Questions, (3) Talk to the PDF: 3\n",
      "You can now talk to the PDF. Ask any questions based on its content.\n",
      "Your question (type 'exit' to quit): when was the degree completed\n",
      "Response:\n",
      "The provided text does not specify when the degree was completed, so I cannot answer this question from the given context. Mar 2024\n",
      "\n",
      "Your question (type 'exit' to quit): which school\n",
      "Response:\n",
      "The provided text does not mention any schools, so I cannot answer this question. ihub divyasampark iit roorkee\n",
      "\n",
      "Your question (type 'exit' to quit): which project\n",
      "Response:\n",
      "The provided text does not include any specific project names, so I cannot answer this question from the provided context. AI-Based Multilingual Course Generator\n",
      "\n",
      "Your question (type 'exit' to quit): exit\n",
      "Do you want to perform another operation? (yes/no): no\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import multiprocessing\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "import warnings\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "import markdown\n",
    "import time\n",
    "import urllib.parse\n",
    "from googletrans import Translator, LANGUAGES\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure the Google Generative AI API\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def split_text_into_chunks(text, max_chunk_size=2000):\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) + 1 > max_chunk_size:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                current_chunk += \". \" + sentence\n",
    "            else:\n",
    "                current_chunk = sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def generate_content(prompt, max_tokens=1024, retries=3, wait=5):\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            if response.parts:\n",
    "                return response.parts[0].text.strip()\n",
    "            else:\n",
    "                raise ValueError(\"No valid parts in response.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Retrying in {wait} seconds...\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                return f\"Failed to generate content after {retries} attempts.\"\n",
    "\n",
    "def create_prompts(text, task_type):\n",
    "    prompts = {\n",
    "        \"mcq\": f\"Read the following text carefully and generate multiple-choice questions. Each question should include:\\n\"\n",
    "               f\"1. A clear and concise question based on the text.\\n\"\n",
    "               f\"2. Four answer options (A, B, C, D), with one correct answer clearly indicated.\\n\"\n",
    "               f\"3. The questions should cover key concepts, definitions, critical points, and significant details discussed in the text.\\n\"\n",
    "               f\"4. Ensure the options are plausible and relevant to the content.\\n\\n\"\n",
    "               f\"Text:\\n{text}\\n\\nMCQ:\",\n",
    "        \"fill_in_the_blank\": f\"Read the following text thoroughly and generate fill-in-the-blank questions. Each question should include:\\n\"\n",
    "                            f\"1. A sentence from the text with one key term or concept replaced by a blank.\\n\"\n",
    "                            f\"2. The correct term or concept that completes the sentence accurately.\\n\"\n",
    "                            f\"3. Focus on important information, such as key terms, dates, names, and concepts that are critical to understanding the text.\\n\\n\"\n",
    "                            f\"Text:\\n{text}\\n\\nFill in the blank:\",\n",
    "        \"short_answer\": f\"Read the following text attentively and generate short answer questions. Each question should include:\\n\"\n",
    "                        f\"1. A clear and specific question that requires a brief response.\\n\"\n",
    "                        f\"2. The response should address key points, explanations, or definitions provided in the text.\\n\"\n",
    "                        f\"3. Ensure the questions encourage critical thinking and comprehension of the material, focusing on important details and concepts.\\n\\n\"\n",
    "                        f\"Text:\\n{text}\\n\\nShort answer question:\",\n",
    "        \"course\": f\"Read the following text and generate a comprehensive, structured course content. The content should include:\\n\"\n",
    "                  f\"1. Detailed learning objectives and outcomes for each chapter, helping students understand what they are expected to learn.\\n\"\n",
    "                  f\"2. A chapter-wise breakdown with detailed descriptions and subtopics, summarizing the main points for easy study.\\n\"\n",
    "                  f\"3. Key concepts, definitions, and explanations for each chapter, highlighting the essential information.\\n\"\n",
    "                  f\"4. Examples, illustrations, and case studies relevant to each chapter, to help students grasp practical applications.\\n\"\n",
    "                  f\"5. Practical exercises and activities for students to reinforce learning, ensuring they can apply what they have learned.\\n\"\n",
    "                  f\"6. Summaries and key takeaways for each chapter, so students can review the main points quickly.\\n\"\n",
    "                  f\"7. Supplementary resources and reading materials for further study, providing avenues for deeper exploration.\\n\"\n",
    "                  f\"8. Guidance notes and tips for teachers on how to effectively deliver the content and engage students.\\n\"\n",
    "                  f\"9. Incorporate interactive elements and multimedia content where possible, to enhance learning and retention.\\n\"\n",
    "                  f\"10. Ensure the curriculum is structured in a logical and progressive manner to facilitate a smooth learning experience.\\n\\n\"\n",
    "                  f\"Text:\\n{text}\\n\\nComprehensive Course Content:\",\n",
    "        \"chat\": f\"Use the following text as a reference to answer questions based on its content.\\n\\n\"\n",
    "                f\"Text:\\n{text}\\n\\nQuestion:\",\n",
    "    }\n",
    "    return prompts.get(task_type, \"\")\n",
    "\n",
    "\n",
    "\n",
    "def get_user_input(prompt):\n",
    "    return input(prompt)\n",
    "\n",
    "def translate_text_google_translate(text, target_language):\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(text, dest=target_language)\n",
    "    return translation.text\n",
    "\n",
    "def save_to_json(filename, data):\n",
    "    with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_from_json(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as json_file:\n",
    "            return json.load(json_file)\n",
    "    return {}\n",
    "\n",
    "def talk_to_pdf(text_chunks):\n",
    "    print(\"You can now talk to the PDF. Ask any questions based on its content.\")\n",
    "    while True:\n",
    "        user_question = get_user_input(\"Your question (type 'exit' to quit): \")\n",
    "        if user_question.lower() == 'exit':\n",
    "            break\n",
    "        responses = []\n",
    "        for chunk in text_chunks:\n",
    "            prompt = create_prompts(chunk, \"chat\") + user_question\n",
    "            response = generate_content(prompt)\n",
    "            responses.append(response)\n",
    "        print(\"Response:\")\n",
    "        print(\" \".join(responses))\n",
    "        print()\n",
    "\n",
    "def chatbot():\n",
    "    pdf_path = get_user_input(\"Enter the path to your PDF file: \")\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    text_chunks = split_text_into_chunks(pdf_text)\n",
    "\n",
    "    while True:\n",
    "        main_choice = get_user_input(\"Choose an option: (1) Generate Course, (2) Generate Questions, (3) Talk to the PDF: \").lower()\n",
    "        if main_choice not in ['1', '2', '3']:\n",
    "            print(\"Invalid choice. Please choose '1', '2', or '3'.\")\n",
    "            continue\n",
    "\n",
    "        if main_choice == '1':\n",
    "            prompt = create_prompts(pdf_text, \"course\")\n",
    "            course_outline = generate_content(prompt)\n",
    "            print(\"Course Outline and Lesson Plan (in English):\")\n",
    "            print(course_outline)\n",
    "            save_to_json('course_outline.json', {\"course_outline\": course_outline})\n",
    "\n",
    "            translate_choice = get_user_input(\"Do you want to translate the course outline? (yes/no): \").lower()\n",
    "            if translate_choice == 'yes':\n",
    "                languages = {v: k for k, v in LANGUAGES.items()}  # Reverse lookup\n",
    "                print(\"Available languages:\")\n",
    "                for code, language in languages.items():\n",
    "                    print(f\"{language} ({code})\")\n",
    "                language_choice = get_user_input(\"Enter the language code you want to translate to: \")\n",
    "                if language_choice in languages:\n",
    "                    translated_outline = translate_text_google_translate(course_outline, language_choice)\n",
    "                    print(f\"Translated Course Outline ({languages[language_choice]}):\")\n",
    "                    print(translated_outline)\n",
    "                    save_to_json(f'course_outline_{language_choice}.json', {\"course_outline\": translated_outline})\n",
    "                else:\n",
    "                    print(\"Invalid language code.\")\n",
    "\n",
    "        elif main_choice == '2':\n",
    "            previous_question_type = None\n",
    "            all_questions = load_from_json('questions.json')\n",
    "            while True:\n",
    "                question_type = get_user_input(\"Choose the type of questions to generate (mcq, fill_in_the_blank, short_answer): \").lower()\n",
    "                if question_type not in ['mcq', 'fill_in_the_blank', 'short_answer']:\n",
    "                    print(\"Invalid choice. Please choose either 'mcq', 'fill_in_the_blank', or 'short_answer'.\")\n",
    "                    continue\n",
    "                if question_type == previous_question_type:\n",
    "                    print(f\"You've already generated {question_type} questions. Please choose a different type.\")\n",
    "                    continue\n",
    "\n",
    "                num_questions = int(get_user_input(\"Enter the number of questions to generate (5, 10, 15): \"))\n",
    "                if num_questions not in [5, 10, 15]:\n",
    "                    print(\"Invalid number of questions. Please choose either 5, 10, or 15.\")\n",
    "                    continue\n",
    "\n",
    "                questions = []\n",
    "                for chunk in text_chunks:\n",
    "                    prompt = create_prompts(chunk, question_type)\n",
    "                    question = generate_content(prompt)\n",
    "                    questions.append(question)\n",
    "\n",
    "                all_questions[question_type] = questions[:num_questions]\n",
    "                save_to_json('questions.json', all_questions)\n",
    "\n",
    "                print(f\"{question_type.upper()} Questions (in English):\")\n",
    "                for question in questions[:num_questions]:\n",
    "                    print(question)\n",
    "                    print()\n",
    "\n",
    "                translate_choice = get_user_input(\"Do you want to translate the questions? (yes/no): \").lower()\n",
    "                if translate_choice == 'yes':\n",
    "                    languages = {v: k for k, v in LANGUAGES.items()}  # Reverse lookup\n",
    "                    print(\"Available languages:\")\n",
    "                    for code, language in languages.items():\n",
    "                        print(f\"{language} ({code})\")\n",
    "                    language_choice = get_user_input(\"Enter the language code you want to translate to: \")\n",
    "                    if language_choice in languages:\n",
    "                        translated_questions = []\n",
    "                        for question in questions[:num_questions]:\n",
    "                            translated_question = translate_text_google_translate(question, language_choice)\n",
    "\n",
    "                            translated_questions.append(translated_question)\n",
    "                            print(f\"Translated {question_type.upper()} Question ({languages[language_choice]}):\")\n",
    "                            print(translated_question)\n",
    "                        all_questions[f\"{question_type}_{language_choice}\"] = translated_questions\n",
    "                        save_to_json('questions.json', all_questions)\n",
    "                    else:\n",
    "                        print(\"Invalid language code.\")\n",
    "\n",
    "                another_round = get_user_input(\"Do you want to generate a different type of questions? (yes/no): \").lower()\n",
    "                if another_round != 'yes':\n",
    "                    break\n",
    "                previous_question_type = question_type\n",
    "\n",
    "        elif main_choice == '3':\n",
    "            talk_to_pdf(text_chunks)\n",
    "\n",
    "        another_main_round = get_user_input(\"Do you want to perform another operation? (yes/no): \").lower()\n",
    "        if another_main_round != 'yes':\n",
    "            break\n",
    "\n",
    "# Start the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
